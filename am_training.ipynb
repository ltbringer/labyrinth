{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition\n",
    "\n",
    "\n",
    "Here we will try to use minimal amount of speech data to build a speech-recognition model and learn how much data matters, also the type of it: \n",
    "\n",
    "- How much does equal phonemic distribution matter? \n",
    "- How much variation in speakers is required?\n",
    "- What is a good-enough model?\n",
    "\n",
    "We want to model acoustic behaviour of human speech. To do that, we will:\n",
    "\n",
    "1. Pick up some audios from any open source data-set, place it in `./data/raw`.\n",
    "2. Tag them with phonemes.\n",
    "3. Split each audio file such that it contains 250ms worth of audio, if the audio runs out of data, we must add padding and save all such audios in `./data/phonemes`.\n",
    "4. Each 250ms chunk audio file is labelled to contain a single phoneme.\n",
    "5. We will also window it by 50ms. (Kaldi does 20-25ms)\n",
    "6. If a frame contains no phonemes, we will mark it with SIL for silence.\n",
    "\n",
    "```\n",
    "Chunks:\n",
    "|-----------------------------------------------|\n",
    "|<----------- total audio (400ms) ------------->|\n",
    "\n",
    "|-----------------------------| [chunk:1](file__symA__1.wav)\n",
    "         |-----------------------------| [chunk:2](file__symB__2.wav)\n",
    "                  |-----------------------------| [chunk:3](file__symC__3.wav)\n",
    "|< 50ms >|\n",
    "|<----------- 250ms --------->|\n",
    "```\n",
    "Here, we can see the conventions we are about to follow, a given `file` is split into 3 chunks\n",
    "by our frame @250ms and window @50ms sizes, also we create new files with the name same as the original file, the phoneme contained in the frame and lastly the chunk number of the file all joined by `__` to give:\n",
    "\n",
    "```\n",
    "(file__symA__1.wav)\n",
    "```\n",
    "Once such files are crated, we will prepare `pytorch` [IterableDataset](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset). There is another [guide](https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd) that can be followed for a more detailed review on the topic.\n",
    "\n",
    "Finally, we will train a phoneme decoder using CNN-GRU architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:33:51.809138Z",
     "start_time": "2020-02-02T15:33:51.228314Z"
    }
   },
   "outputs": [],
   "source": [
    "# In case we need to save some artifacts\n",
    "import pickle\n",
    "# To utilize cores for data tasks (split audios and save)\n",
    "import multiprocessing as mp\n",
    "# Type check\n",
    "from typing import List, Dict, Callable\n",
    "# Get file paths\n",
    "from glob import glob\n",
    "\n",
    "# We should avoid np as much as possible and use torch since\n",
    "# we have to use torch for models anyway.\n",
    "import numpy as np\n",
    "# NN models\n",
    "import torch\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Control cells that must not always run\n",
    "import ipywidgets as widgets\n",
    "# read-write wave audios\n",
    "from scipy.io import wavfile\n",
    "# Since we will be training on spectrograms\n",
    "from scipy.signal import spectrogram\n",
    "# Play audios (analysis)\n",
    "from IPython.display import Audio\n",
    "# To not worry about vis legibility since we are using a dark theme.\n",
    "from jupyterthemes import jtplot\n",
    "# Building iterable dataset\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:33:53.298203Z",
     "start_time": "2020-02-02T15:33:53.285246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set matplotlib to go along the theme\n",
    "jtplot.style(theme='solarizedd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:33:56.455823Z",
     "start_time": "2020-02-02T15:33:56.453297Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAME_SIZE = 250\n",
    "WINDOW = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "This section will be devoted to splitting audio files by phonemes and saving them in chunks of 250ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio file chunk creation\n",
    "\n",
    "Audio files are assumed to be saved in \"./data/raw/\" dir. Prepare phoneme tags via Audacity or\n",
    "any open source audio-text aligners, save the result in this format:\n",
    "\n",
    "| start time | end time | phoneme |\n",
    "| ---------- | -------- | ------- |\n",
    "|  0.100     |  0.150   |  ah_S   |\n",
    "\n",
    "and name it as `<file>_labels.txt` in the \"./data/raw/\" dir.\n",
    "\n",
    "Now we will break each audio into `FRAME_SIZE` parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:04.523164Z",
     "start_time": "2020-02-02T15:34:04.509658Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_padded_last_timestamp(timestamp: float) -> float:\n",
    "    \"\"\"\n",
    "    Extrapolate a number to fit within the frame-size.\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    Let, timestamp be 5.120s. This falls short of the FRAME_SIZE(250ms)\n",
    "    for its last chunk. So we want to fix it such that the last chunk matches\n",
    "    the frame-size.\n",
    "    \n",
    "    >> 5120 - 5000 => 120 (can be covered in 1 extra frame)\n",
    "    >> 120//250 + 1 (gives us the number of extra frames)\n",
    "    >> 250 * (120//250 + 1) (gives us the correction value to be added to timestamp)\n",
    "    \"\"\"\n",
    "    diff = (timestamp - int(timestamp)) * 1000\n",
    "    if not diff:\n",
    "        return timestamp\n",
    "    padding = diff//FRAME_SIZE + 1\n",
    "    return (int(timestamp) + (FRAME_SIZE * padding)/1000) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:04.896306Z",
     "start_time": "2020-02-02T15:34:04.876631Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_phoneme_timestamps(audio_label_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Read audio label, return phoneme information for each frame.\n",
    "    \n",
    "    audio_labels contain information in this format:\n",
    "    | start time | end time | phoneme |\n",
    "    | ---------- | -------- | ------- |\n",
    "    |  0.100     |  0.150   |  ah_S   |\n",
    "    \n",
    "    We want to return:    \n",
    "    [{\n",
    "        \"start\": 0,\n",
    "        \"end\": 250,\n",
    "        \"phoneme\": SIL\n",
    "    }, {\n",
    "        \"start\": 0.025,\n",
    "        \"end\": 0.275,\n",
    "        \"phoneme\": \"ah_S\"\n",
    "    }, ...]\n",
    "    \"\"\"\n",
    "    with open(audio_label_path, \"r\") as f:\n",
    "        phoneme_timestamps = f.read().splitlines()\n",
    "    \n",
    "    phoneme_timestamps_ = []\n",
    "    starts, ends, phonemes = zip(*[t.split(\"\\t\") for t in phoneme_timestamps])\n",
    "    \n",
    "    last_timestamp = float(ends[-1])\n",
    "    padded_last_timestamp = int(get_padded_last_timestamp(last_timestamp))\n",
    "\n",
    "    for i in range(0, padded_last_timestamp, WINDOW):\n",
    "        start_time = i/1000\n",
    "        end_time = (i + FRAME_SIZE)/1000\n",
    "        sym = None\n",
    "        for start, end, phoneme in zip(starts, ends, phonemes):\n",
    "            if float(end) > end_time:\n",
    "                break\n",
    "            else:\n",
    "                sym = phoneme\n",
    "            \n",
    "        phoneme_timestamps_.append({\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time,\n",
    "            \"phoneme\": sym or \"SIL\"\n",
    "        })\n",
    "\n",
    "    return phoneme_timestamps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:05.168435Z",
     "start_time": "2020-02-02T15:34:05.155362Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_phoneme_frames(audio_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Break the audio at `audio_path` into frames of FRAME_SIZE.\n",
    "    audio files are assumed to be placed in \"./data/raw/\" dir.\n",
    "    chunked audio files are placed in \"./data/phonemes/\" dir.\n",
    "    \n",
    "    Files with oos phonemes or missing phoneme labels are skipped.\n",
    "    \"\"\"\n",
    "    audio_name = audio_path.replace(\".wav\", \"\").rsplit(\"/\")[-1]\n",
    "    audio_label_path = f\"data/raw/{audio_name}_labels.txt\"\n",
    "    try:\n",
    "        phoneme_timestamps = get_phoneme_timestamps(audio_label_path)\n",
    "        sr, wav = wavfile.read(audio_path)\n",
    "        data_size = int((sr * FRAME_SIZE)/1000)\n",
    "        for i, phoneme_timestamp in enumerate(phoneme_timestamps):\n",
    "            phoneme = phoneme_timestamp[\"phoneme\"]\n",
    "            if \"oov\" not in phoneme:\n",
    "                start = int(phoneme_timestamp[\"start\"] * sr)\n",
    "                end =  int(phoneme_timestamp[\"end\"] * sr)\n",
    "                data = wav[start:end]\n",
    "                if len(data) < data_size:\n",
    "                    data = np.pad(data, (0, data_size - len(data)), 'constant', constant_values=(0))\n",
    "                wavfile.write(f\"data/phonemes/{audio_name}__{phoneme}__{i + 1}.wav\", sr, data)        \n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:05.391427Z",
     "start_time": "2020-02-02T15:34:05.385797Z"
    }
   },
   "outputs": [],
   "source": [
    "def mp__save_phoneme_frames(fn: Callable, data: List[str], workers: int = None) -> None:\n",
    "    workers = workers or mp.cpu_count()\n",
    "    pool = mp.Pool(workers)\n",
    "    _ = pool.map(save_phoneme_frames, audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:05.680058Z",
     "start_time": "2020-02-02T15:34:05.647841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011a5eb270b14fb4a5c60f4539573bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Chunk audios', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f044147b49e42a79d62f1640f9df017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_chunk_btn = widgets.Button(description=\"Chunk audios\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(audio_chunk_btn, output)\n",
    "audio_files = glob(\"data/raw/*.wav\")\n",
    "\n",
    "def on_audio_chunk(btn):\n",
    "    with output:\n",
    "        mp__save_phoneme_frames(save_phoneme_frames, audio_files)\n",
    "\n",
    "audio_chunk_btn.on_click(on_audio_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch IterableDataset\n",
    "\n",
    "An iterable Dataset.\n",
    "\n",
    "- All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.\n",
    "- All subclasses should overwrite `__iter__()`, which would return an iterator of samples in this dataset.\n",
    "\n",
    "When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When `num_workers > 0`, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. `get_worker_info()`, when called in a worker process, returns information about the worker. It can be used in either the dataset’s `__iter__()` method or the `DataLoader` ‘s `worker_init_fn` option to modify each copy’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:06.720177Z",
     "start_time": "2020-02-02T15:34:06.709697Z"
    }
   },
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"\n",
    "    Does what np.split_array(arr, chunk) does, but lazily!\n",
    "    \n",
    "    Let, iterable = [1, 2, 3, 4, 5]\n",
    "    >> grouper(iterable, 2)\n",
    "    >> [(1, 2), (3, 4), (5, None)]\n",
    "    \n",
    "    Tried grouper(list(range(200000)), 2)\n",
    "    returns in 51ms.\n",
    "    \"\"\"\n",
    "    # Create a list with the iterable repeated n times.\n",
    "    args = [iter(iterable)] * n\n",
    "    \n",
    "    # zip will bunch together n elements\n",
    "    # zip_longest, additionally will take a lone chunk\n",
    "    # that zip would have discarded, and replaces None with fillvalue\n",
    "    return zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:07.172530Z",
     "start_time": "2020-02-02T15:34:07.164192Z"
    }
   },
   "outputs": [],
   "source": [
    "def wav_to_spectrogram(self, file: str):\n",
    "    \"\"\"\n",
    "    Given a path to a wav file,\n",
    "    Return a spectrogram as np.ndarray\n",
    "    \"\"\"\n",
    "    sr, data = wavfile.read(file)\n",
    "    _, _, specgram = spectrogram(data, Fs=sr)\n",
    "    return specgram\n",
    "\n",
    "\n",
    "def lazy_load_files(self, files: List[str]):\n",
    "    \"\"\"\n",
    "    For each wav file in a list,\n",
    "    return their spectrogram within an iterator.\n",
    "    \"\"\"\n",
    "    return map(wav_to_spectrogram, files)\n",
    "\n",
    "\n",
    "def stream_batch(self, batched_files: List[str]):\n",
    "    \"\"\"\n",
    "    Wrapper around lazy_load_files to implement\n",
    "    batches as per pytorch's IterableDataset spec.\n",
    "    \"\"\"\n",
    "    return map(self.lazy_load_files, batched_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:07.597318Z",
     "start_time": "2020-02-02T15:34:07.572817Z"
    }
   },
   "outputs": [],
   "source": [
    "class IterableAudioStreamDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data_path: str = None, batch_size: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Setup the instance with all the assets that can be used by __iter__.\n",
    "        We also do validations here.\n",
    "        \"\"\"\n",
    "        super(IterableAudioStreamDataset).__init__()\n",
    "        self.files = glob(f\"{data_path}/*.wav\")\n",
    "        self.batch_size = batch_size\n",
    "        assert self.files, f\"Not enough files at path {data_path}\\nIs there a typo?\"\n",
    "        assert self.batch_size > 0, \"Batch size should be greated than 0!\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Since we could leverage multiple cpu cores to fetch data\n",
    "        from an IterableDataset. \n",
    "        \n",
    "        Why? Since this loader will be returning\n",
    "        data as required instead of loading everything in memory, it will \n",
    "        slow down training speed, even if GPUs are involved.\n",
    "        \n",
    "        If the DataLoader instructs to use `num_workers` to fetch data.\n",
    "        A multiprocessing job will handle the distribution.\n",
    "        \"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        start_idx = 0\n",
    "        # This would be used if we have `num_workers = 1`\n",
    "        # We want to slice our list of files such that \n",
    "        # all batches are read by the single worker.\n",
    "        files_per_batch = int(np.ceil(len(self.files)/self.batch_size))\n",
    "        end_idx = files_per_batch\n",
    "        if worker_info is not None:\n",
    "            # If we have `num_workers > 1` then we need to set\n",
    "            # indices to slice our list of files such that there is \n",
    "            # no duplication of data.\n",
    "            \n",
    "            # so if we have ...\n",
    "            n_workers = worker_info.num_workers\n",
    "            \n",
    "            # This should be the number of files per worker\n",
    "            work_load = len(self.files) // n_workers\n",
    "            # So, we will index each slice by worker id\n",
    "            worker_id = worker_info.id\n",
    "            # such that each worker starts...\n",
    "            start_idx = worker_id * work_load            \n",
    "            # and ends a slice without duplication.\n",
    "            end_idx = start_idx + work_load\n",
    "        return stream_batch(grouper(self.files[start_idx:end_idx], self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:34:08.288155Z",
     "start_time": "2020-02-02T15:34:07.931439Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = IterableAudioStreamDataset(data_path=\"./data/phonemes\", batch_size=5)\n",
    "# We are handling the batch_size withing the dataset, so we have to inform\n",
    "# the DataLoader to peace out for a bit.\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=None, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We will be building a CNN-GRU model.\n",
    "\n",
    "- CNN: To identify phonemes in a spectrogram\n",
    "- GRU: Remember information across sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
